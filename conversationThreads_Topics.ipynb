{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conversationThreads_Topics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHI5Oy9-SaAs",
        "colab_type": "text"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sqtH7pZ-bv6",
        "colab_type": "text"
      },
      "source": [
        "##  For conversation threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHrFVPy5sgc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import requests\n",
        "from pandas.io.json import json_normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFsxrbASnsJ",
        "colab_type": "text"
      },
      "source": [
        "## For topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SW50L5kvHht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import gensim\n",
        "lemmatize = WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words('english'))\n",
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk.data\n",
        "#sid = SentimentIntensityAnalyzer()\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_AtNKMLStp0",
        "colab_type": "text"
      },
      "source": [
        "# List of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B1-WQIns09O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FMlinkList = {'Dec17':'http://educatorinnovator.org/wp-content/uploads/2017/10/Critical-Literacy-And-Our-Students-Lives.pdf',\n",
        "             'Mar18':'http://educatorinnovator.org/wp-content/uploads/2018/02/The-Stories-They-Tell-.pdf'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lER4DL_S2el",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owIbw9HQtECB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build dataframe from hypothes.is API\n",
        "def H_API(session,uri):\n",
        "    n = 0\n",
        "    payload = {'url':uri}\n",
        "    r = requests.get('https://hypothes.is/api/search',params=payload)\n",
        "    tmp = pd.DataFrame(columns=['created', 'document.title', 'group', 'id', 'links.html',\n",
        "       'links.incontext', 'links.json', 'permissions.admin',\n",
        "       'permissions.delete', 'permissions.read', 'permissions.update',\n",
        "       'references', 'tags', 'target', 'text', 'updated', 'uri', 'user'])\n",
        "    total = r.json()['total']\n",
        "    print(session)\n",
        "    print(uri)\n",
        "    print('Number of Records Found for this Session: ',total)\n",
        "    print('Scraped:')\n",
        "    while n < total:\n",
        "        print(n)\n",
        "        tmp = tmp.append(json_normalize(r.json()['rows']))\n",
        "        n = len(tmp)\n",
        "        payload = {\n",
        "            'url':uri,\n",
        "            'offset':n}\n",
        "        r = requests.get('https://hypothes.is/api/search',params=payload)\n",
        "    print(n)\n",
        "    tmp['session'] = session\n",
        "    tmp['updated'] = pd.to_datetime(tmp['updated'])\n",
        "    tmp['created'] = pd.to_datetime(tmp['created'])\n",
        "    tmp = tmp.set_index(tmp['created'])\n",
        "    return tmp\n",
        "\n",
        "#returns the number of times a message was in a reference list of other messages\n",
        "def countReplies(msgId):\n",
        "    return len(df.dropna()[df['references'].dropna().map(lambda x: msgId in x)])\n",
        "def listOfIds(msgId):\n",
        "    return df.dropna()[df['references'].dropna().map(lambda x: msgId in x)]['id'].values\n",
        "def replyTimeDelta(x):\n",
        "    if len(df[df['id']==x]['created']) == 0:\n",
        "      return None\n",
        "    else:\n",
        "      return df[df['id']==x]['created'].iloc[0]\n",
        "\n",
        "#For finding topics\n",
        "def cleaning(article):\n",
        "    one = \" \".join([i for i in article.lower().split() if i not in stopwords])\n",
        "    two = \"\".join(i for i in one if i not in punctuation)\n",
        "    three = \" \".join(lemmatize.lemmatize(i) for i in two.split())\n",
        "    return three\n",
        "  \n",
        "def labelizer(x):\n",
        "    labels1 = ''.join(x[0][1].split()[::2]).split('\"')[1::2]\n",
        "    labels2 = ''.join(x[1][1].split()[::2]).split('\"')[1::2]\n",
        "    singleLabel = list(labels1[:2]+labels2[:2])\n",
        "    return singleLabel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mtm2mzvTAnN",
        "colab_type": "text"
      },
      "source": [
        "# Create dataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi6zr0WN-rT1",
        "colab_type": "text"
      },
      "source": [
        "## With hypothes.is data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6lQuWbAtE6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['created','document.title','group','id','links.html',\n",
        "       'links.incontext','links.json','permissions.admin',\n",
        "       'permissions.delete','permissions.read','permissions.update',\n",
        "       'references','tags','target','text','updated','uri','user','session'])\n",
        "for session,link in FMlinkList.items():\n",
        "    df = df.append(H_API(session,link))\n",
        "df = df.tz_localize('UTC').tz_convert('US/Mountain')\n",
        "print('Total Records: ',len(df))\n",
        "\n",
        "#Modify df for network analysis\n",
        "df = df[['created','id','references','session','tags','target', 'text','user']]\n",
        "df['replyTo'] = df['references'].map(lambda x: x[-1],na_action='ignore')\n",
        "df['user'] = df['user'].map(lambda x: x[5:-12])\n",
        "\n",
        "#Time betweeen message and reply\n",
        "df['replyDelay'] = df[df['replyTo'].notnull()]['created']\\\n",
        "- df[df['replyTo'].notnull()]['replyTo'].map(lambda x: replyTimeDelta(x))\n",
        "\n",
        "#convert to minutes\n",
        "df['replyDelay'] = df['replyDelay'].map(lambda x: x.total_seconds() / 60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_awI0vFPTYf-",
        "colab_type": "text"
      },
      "source": [
        "## Build network of threads (2 or more messages in conversation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypoz_gRtlVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = nx.from_pandas_edgelist(df[df['references'].notnull()],'id','replyTo'\n",
        "                           ,edge_attr=['tags','target','text','user','replyDelay']\n",
        "                           ,create_using=nx.DiGraph())\n",
        "\n",
        "# Build dataframe of threads with list of ids per thread\n",
        "threads = []\n",
        "for x in nx.connected_components(nx.to_undirected(G)):\n",
        "    threads.append(x)\n",
        "\n",
        "# list of text per thread\n",
        "textT = []\n",
        "timesT = []\n",
        "usersT = []\n",
        "for thread in threads:    \n",
        "    T = []\n",
        "    t = []\n",
        "    u = []\n",
        "    for key,val in df[df['id'].isin(thread)].iterrows():\n",
        "        T.append(val['text'])\n",
        "        t.append(val['replyDelay'])\n",
        "        u.append(val['user'])\n",
        "    textT.append(T)\n",
        "    timesT.append(t)\n",
        "    usersT.append(u)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DkO45mvToC6",
        "colab_type": "text"
      },
      "source": [
        "## Build 'threads' dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqlv35lvtH7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfThreads = pd.DataFrame(pd.Series(data=threads),columns=['ids'])\n",
        "dfThreads['texts'] = pd.Series(textT)\n",
        "dfThreads['timeDelays'] = pd.Series(timesT)\n",
        "dfThreads['users'] = pd.Series(usersT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3QwXCNA-6v2",
        "colab_type": "text"
      },
      "source": [
        "## Add topics and labels to thread df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE-aKKUU-4-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topicsList = []\n",
        "for key,val in dfThreads.iterrows():\n",
        "    txt = [cleaning(c) for c in val['texts']]\n",
        "    text_list = [a.split() for a in txt]\n",
        "    dictionary = gensim.corpora.Dictionary(text_list)\n",
        "    corpus = [dictionary.doc2bow(text2) for text2 in text_list]\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=2,id2word=dictionary,passes=20)\n",
        "    topicsList.append(ldamodel.print_topics(num_topics=2,num_words=5))\n",
        "\n",
        "dfThreads['topics'] = topicsList\n",
        "dfThreads['labels'] = dfThreads['topics'].map(lambda x: labelizer(x))\n",
        "dfThreads['totalMsgs'] = dfThreads['ids'].map(lambda x: len(x))\n",
        "dfThreads['totalMsgsCatagories'] = pd.qcut(dfThreads['totalMsgs'],4,duplicates='drop')\n",
        "dfThreads = dfThreads.sort_values(by='totalMsgs',ascending=False)\n",
        "dfThreads.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBJuUkmOUCLR",
        "colab_type": "text"
      },
      "source": [
        "# Text output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXTxwySq_Cxi",
        "colab_type": "text"
      },
      "source": [
        "## Size and topics of individual threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vbwDoF31MlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key,vals in dfThreads.iterrows():\n",
        "  print('Message Count: ',vals['totalMsgs'])\n",
        "  print('Participants :', list(set(vals['users'])))\n",
        "  print('Topics: ',vals['labels'])\n",
        "  print('Text: ', vals['texts'])\n",
        "  print('***')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-owDDCFUGiH",
        "colab_type": "text"
      },
      "source": [
        "## Connected conversations as network graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58FPKYkms0Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key,val in dfThreads.iterrows():\n",
        "  ## To display with message ID\n",
        "  #nx.draw_spring(G.subgraph(i),with_labels=True)\n",
        "  ## To display with username\n",
        "  nx.draw(G.subgraph(val['ids']),labels={val2['id']:val2['user'] for \n",
        "                                key2,val2 in df[df['id'].isin(val['ids'])].iterrows()})\n",
        "  plt.title(val['labels'])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KMoBSGeH55O",
        "colab_type": "text"
      },
      "source": [
        "##Threads with 3 or more messages \n",
        "###Prints text by participants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A3Ah0TKDOUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key,vals in dfThreads.iterrows():\n",
        "  if (vals['totalMsgs'] < 3): continue\n",
        "  print(\"**************************************************\")\n",
        "  print('Message Count: ',vals['totalMsgs'])\n",
        "  print(\"**************************************************\")\n",
        "  for p in zip(vals['users'], vals['texts']):\n",
        "    print(p[0]+\":\",p[1])\n",
        "    print(\"-------------------\")\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}